{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "\n",
        "<div>\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Javeriana.svg/1200px-Javeriana.svg.png\" width=\"500\"/>\n",
        "</div>\n",
        "\n",
        "# Facultad de Ingeniería Industrial\n",
        "## Maestria en Analítica en Inteligencia de Negocio\n",
        "### Materia: Tópicos Avanzados en Analítica\n",
        "### Modúlo: Procesamiento de Lenguaje Natural"
      ],
      "metadata": {
        "id": "R36i0tfEz4Vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenización\n",
        "\n",
        "En NLP el proceso de convertir nuestras secuencias de caracteres, palabras o párrafos en inputs para la computadora se llama **tokenización**. Se puede pensar al token como la unidad para procesamiento semántico."
      ],
      "metadata": {
        "id": "Nfb0e7Js1dEv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv7vRwTzzXvH",
        "outputId": "fb556b21-8ec9-4d67-cae7-51ceb5a72ff7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana?']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "tk = WhitespaceTokenizer()\n",
        "texto = \"¿Cuánto tiempo pasó desde que comí una manzana?\"\n",
        "texto_tokenizado = tk.tokenize(texto)\n",
        "print(texto_tokenizado)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "como vemos *manzana* y *cuánto* figuran con el signo de pregunta. Y si tuvieramos la palabra manzana mencionada otra vez saldría nuevamente como un token diferente. Lo mismo nos sucedería si aparece una coma o un punto ¿Cómo hacemos para evitarlo?"
      ],
      "metadata": {
        "id": "FbM9INzk3HWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos utilizar `TreebankWordTokenizer` en lugar de `WhitespaceTokenizer`. También podemos preprocesar el texto quitando comas y signos de puntuación y separar por espacios, o bien utilizar opciones como `WordPunctTokenizer` que separa por palabras tomando como separadores todo lo que no sea un caracter alfabetico."
      ],
      "metadata": {
        "id": "pR4uGVu73kgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "texto = \"¿Cuánto tiempo pasó desde que comí una manzana?\"\n",
        "texto_tokenizado1 = WordPunctTokenizer().tokenize(texto)\n",
        "texto_tokenizado2 = TreebankWordTokenizer().tokenize(texto)"
      ],
      "metadata": {
        "id": "woH8Vq5W1bwp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_tokenizado1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6D2WNuV18eX",
        "outputId": "da7308dd-c3c0-4624-8281-87e1448e0f42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¿', 'Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_tokenizado2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAMX05Gx1_qW",
        "outputId": "8ce3dfc5-41e4-4b35-cc35-8a35b2c94595"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¿Cuánto', 'tiempo', 'pasó', 'desde', 'que', 'comí', 'una', 'manzana', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como evidenciamos la opción de `TreebankWordTokenizer` es la más popular en inglés el signo de apertura de pregunta \"¿\" fue un problema para ella. Mientras que la opción de WordPunctTokenizer no tuvo ningún problema."
      ],
      "metadata": {
        "id": "Pi0PPMN038Qc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿cómo crees que se comportarían frente a una apóstrofe?"
      ],
      "metadata": {
        "id": "NpqVpFFY4gxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trabajo de Grupo\n",
        "\n",
        "#INTEGRANTES\n",
        "\n",
        "1. SONIA RAMÍREZ\n",
        "2. MILI GALINDO\n",
        "3. ANGUIE GARCÍA\n",
        "4. LOURDES RODIL"
      ],
      "metadata": {
        "id": "nEtbJ-17_v9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Don't worry be happy\"\n",
        "texto_tokenizado3 = WordPunctTokenizer().tokenize(texto)\n",
        "texto_tokenizado4 = TreebankWordTokenizer().tokenize(texto)"
      ],
      "metadata": {
        "id": "i3LHqWjY2ATa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_tokenizado3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC4KxIEDDhBV",
        "outputId": "7f34ca9d-2e6d-4f92-d70c-ffbdf7578972"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Don', \"'\", 't', 'worry', 'be', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa que cuando usamos WordPunctTokenizer en palabras con apóstrofes en inglés, el WordPunctTokenizer los trata como tokens individuales y los separa del resto de la palabra."
      ],
      "metadata": {
        "id": "Vy3963xiDne9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_tokenizado4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37caOR_rEjtW",
        "outputId": "f04254c1-0d4b-4fde-af39-9c5484bc7590"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Do', \"n't\", 'worry', 'be', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia del WordPunctTokenizer, el TreebankWordTokenizer implementa reglas más específicas para tokenizar palabras en inglés. En nuestro caso al ser una contracción de una palabra, el TreebankWordTokenizer mantuvo la contracción intacta (n't) como una sola unidad."
      ],
      "metadata": {
        "id": "pejKAf4fE1Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto1 = \"She's bag is red\"\n",
        "texto_tokenizado5 = WordPunctTokenizer().tokenize(texto1)\n",
        "texto_tokenizado6 = TreebankWordTokenizer().tokenize(texto1)"
      ],
      "metadata": {
        "id": "nYmFUZt9FTLT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_tokenizado5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGHAKo6IF120",
        "outputId": "a86794e0-bac0-4304-c5b5-cfa41b4c74e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['She', \"'\", 's', 'bag', 'is', 'red']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(texto_tokenizado6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meXp6RxcGAz0",
        "outputId": "1a029873-1870-423d-b85e-d52e1051e60a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['She', \"'s\", 'bag', 'is', 'red']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa que en el caso de los posesivos, funciona mejor el TreebankWordTokenizer, ya que los trata como una sola unidad y no los divide en partes separadas."
      ],
      "metadata": {
        "id": "5y5_4KPwGFV-"
      }
    }
  ]
}